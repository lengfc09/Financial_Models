{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a5e30402f147a97cac5d558dbfea4ff3f916a58d454bb2c6efc731faf3dd0927"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Ling Arthur'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "# This file contains a few examples of how to use the re module and\n",
    "# how to deal with regular expressions.\n",
    "import re                         #  Import 're' module.\n",
    "re.sub(r'K', r'L', 'King Arthur') #  Replace pattern in string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "re.match(r'abc', 'abcdef')        # Match a substring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'abc'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "re.match(r'abc','abcdef').group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 5), match='Hello'>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "\n",
    "re.match(r'\\w+', 'Hello world!') # Match a word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 25), match='lowercase and nums like 8'>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "re.match(r'[a-z0-9 ]+', 'lowercase and nums like 8, but no commas.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test.']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "re.split(r'\\s+', 'This is a test.') # Returns list split on spaces, e.g. tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Let', 's', 'write', 'regex']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "re.findall(r'\\w+', \"Let's write regex!\") # Find all words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Hello world', \" Let's write regex\", \" Isn't this great\", '']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Split into sentences.\n",
    "re.split(r'[.?!]', \"Hello world! Let's write regex. Isn't this great?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Hello', 'I', 'Hong', 'Kong']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Find all capitalized words.\n",
    "re.findall(r'[A-Z]\\w*', 'Hello world, I love Hong Kong.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['1984', '1949']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "re.findall(r'\\d+', 'The novel 1984 was published in 1949.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['He', 'has', '12', 'cats']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    " # Match digits and words (but not anything else, e.g. punctuation).\n",
    "re.findall('\\d+|\\w+', 'He has 12 cats.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['He', 'has', '12', 'cats']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "re.findall('\\w+', 'He has 12 cats.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = re.search(r'coconuts', 'I love coconuts.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7 15\n"
     ]
    }
   ],
   "source": [
    "print(m.start(), m.end())       # Print start and end indices.\n",
    "# Find square bracket containing a word (but no space or anything else).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'coconuts'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<re.Match object; span=(25, 31), match='[nice]'>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "re.search(r'\\[\\w+\\]', 'Hello [wind bla] this is [nice].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file shows some tokenization examples using the NLTK package.\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Ben/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Hi', 'there', '!']"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "word_tokenize(\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Hello world.', 'I love HK!']"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "sent_tokenize('Hello world. I love HK!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'.', 'HK', 'I', 'NYC', 'love'}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# Make set of unique tokens.\n",
    "set(word_tokenize('I love HK. I love NYC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['SOLDIER', '#1', 'Found', 'them', '?']"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# Tokenize based on regular expression.\n",
    "regexp_tokenize('SOLDIER #1: Found them?', r'(\\w+|#\\d|\\?|!)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['#NLP']"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# remove ? mark\n",
    "# Find hastags in tweets.\n",
    "regexp_tokenize('This is a great #NLP exercise.', r'#\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['#NLP', '$sp500', '@blabla']"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# Find mentions and hashtags.\n",
    "regexp_tokenize('great #NLP exercise $sp500 from @blabla.', r'[#$@]\\w+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()    # Create instance of TweetTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['thanks', '@blabla'], ['#NLP', 'is', 'fun', '!']]"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "[tknzr.tokenize(t) for t in ['thanks @blabla', '#NLP is fun!']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['program', 'program', 'program', 'program', 'program', 'program']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "ps_s=[ps.stem('program'),ps.stem('program'),ps.stem('programs'),ps.stem('programer'),ps.stem('programing'),ps.stem('programers')]\n",
    "\n",
    "ps_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "wnls=[wnl.lemmatize('dogs'),  wnl.lemmatize('churches'),  wnl.lemmatize('aardwolves'),\n",
    "  wnl.lemmatize('abaci') ,   wnl.lemmatize('hardrock')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['dog', 'church', 'aardwolf', 'abacus', 'hardrock']"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "wnls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}