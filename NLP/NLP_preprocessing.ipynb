{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a5e30402f147a97cac5d558dbfea4ff3f916a58d454bb2c6efc731faf3dd0927"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "## Stop word removal: \n",
    "\n",
    "This means we exclude some terms, e.g. “the,” “a,” “is,” “at,” “which,” or “on” (so-called “stop words”). These are words that do not contribute a lot to a deeper understanding of the text in consideration as they do not have significant meaning. You can see programming examples of stop word removals in Section 15.4 starting on page 126 and in Section 19.5 starting on page 146.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Stemming and lemmatization: \n",
    "\n",
    "Here you combine different grammatical forms of the same words, e.g. “travel,” “traveling,” and “traveled.” Again the basic idea is to remove noisy data (in this case the suffix) that are of limited use in understand- ing the text. The difference between stemming and lemmatization is that stem- ming often uses a crude heuristic process, while lemmatization tries to do things properly with the use of a vocabulary and morphological analysis of words. For ex- ample, when considering the token “saw,” stemming might just return “s” whereas lemmatization would attempt to return either “saw” or “see” depending on whether the token is used as a noun or verb. The following example in Python illustrates lemmatization (using the NLTK package; you could also use Pattern, which also has fancy lemmatization).\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Lower case** conversion means you convert all words to lowercase. This can be impor- tant, e.g. some words appear at the beginning of a sentence and are thus capitalized, but their content is the same as their lower case version.\n",
    "\n",
    " **Synonyms**: For example “begin” and “commence” have nearly the same meaning and therefore could be treated the same. You could thus replace all synonyms, e.g. you would replace “commence” and “start” by “begin.” For a given word (such as “begin” in the code below), you can find all its synonyms as follows:\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Begin', 'Menachem_Begin', 'get_down', 'begin', 'get', 'start_out', 'start', 'set_about', 'set_out', 'commence', 'begin', 'start', 'begin', 'lead_off', 'start', 'commence', 'begin', 'begin', 'begin', 'begin', 'start', 'begin', 'start', 'begin', 'begin']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Begin',\n",
       " 'Menachem_Begin',\n",
       " 'begin',\n",
       " 'commence',\n",
       " 'get',\n",
       " 'get_down',\n",
       " 'lead_off',\n",
       " 'set_about',\n",
       " 'set_out',\n",
       " 'start',\n",
       " 'start_out'}"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import wordnet\n",
    "syns = []\n",
    "for s in wordnet.synsets('begin'):\n",
    "    for lem in s.lemmas():\n",
    "        syns.append(lem.name())\n",
    "print(syns)\n",
    "set(syns)"
   ]
  },
  {
   "source": [
    "## Special words: \n",
    "\n",
    "For example “Microsoft Windows” has nothing to do with the com- mon use of the term “Windows.” Or “New York” should really be treated as a single term instead of the two separate words “New” and “York.” If you end up having a lot of these kinds of words in your specific application, you can use named entity recognition (NER) as discussed in Chapter 17 starting on page 135. Alternatively you might consider concatenating them e.g. to “MicrosoftWindows” and “NewYork” to make sure they are treated correctly. Of course, this involves some hand-coding and in many applications might after all not be necessary. But it might make an important difference in some specific applications.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'NewYork and California'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "s = 'New York and California'\n",
    "s.replace('New York', 'NewYork')\n"
   ]
  },
  {
   "source": [
    "## Part of speech tagging \n",
    "\n",
    "(see the example code in Chapter 17 starting on page 135): \n",
    "Each sentence is analyzed and tags are appended to nouns, verbs, etc. For example the sentence “And now for something completely different” becomes:\n",
    "\n",
    "  And/CC now/RB for/IN something/NN completely/RB different/JJ\n",
    "CC is a coordinating conjunction, RB is an adverb, IN is a preposition, NN is a noun, and JJ is an adjective."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Reduce the number of words in your text corpus.\n",
    "– Discard all one- or two-letter words, e.g. “a” or “to.”\n",
    "\n",
    "– Exclude numbers.\n",
    "\n",
    "– For example, you could remove words that occur only in 0.5% or less of all doc- uments. (The threshold level of 0.5% is arbitrary and can be chosen differently depending on your application.) So if a word does not occur in at least 99.5% of all documents, you remove it from your text corpus. To decide whether to remove a given word, you would cycle through all documents, check whether the word occurs in each document, and then divide the number of documents it occurred in by the total number of documents. If this number is less than 0.995, you would remove that word.\n",
    "\n",
    "– Alternatively you could for example only keep the 2000 most commonly-used words and discard the rest. The basic idea is that you can express yourself pretty well if you have a vocabulary of 2000 words (or even less), even though the English language for example has more than 100,000 words in total. Of course, whether you use 2000 as the cutoff depends on your specific application and you might want to use different cutoff levels.\n",
    "\n",
    "– Use a dictionary and only keep words that occur in this dictionary, i.e. remove a word if it does not have a matching entry in the dictionary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Convert the words into n-grams. \n",
    "\n",
    "Consider for example the sentence “I live in HK.” If you set n = 2 (so-called bigrams), you will end up with: (I live), (live in), (in HK). So the basic idea is to not look at single words, but instead to look at combinations of words in order to capture more aspects of the language structure. The advantage of n-grams are that you can take care of word order, e.g. if the text says “not happy,” you could capture the negation in a bigram. The disadvantage is that your vectors have a high dimensionality and tend to be very sparse. The following is an example of bigrams using the NLTK package:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['I live', 'live in', 'in HK', 'HK .']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "s = 'I live in HK.'\n",
    "ngs = ngrams(word_tokenize(s), 2)\n",
    "[' '.join(ng) for ng in ngs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['I live', 'live in', 'in HK']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "#An alternative implementation using TextBlob:\n",
    "from textblob import TextBlob\n",
    "ngs = TextBlob('I live in HK.').ngrams(n=2)\n",
    "[' '.join(ng) for ng in ngs]\n"
   ]
  },
  {
   "source": [
    "In practice you might also consider using the ngram_range argument to Sklearn’s CountVectorizer function. For an example usage of CountVectorizer, see Sec- tion 15.5 starting on page 127."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Tokenization (see also Section 10.4 starting on page 75 for a programming exam- ple):\n",
    "\n",
    "The basic idea is to split up the whole text document into smaller parts, the so-called tokens. \n",
    "\n",
    "• word_tokenize to tokenize text into words.\n",
    "\n",
    "• sent_tokenize to tokenize a document into sentences.\n",
    "\n",
    "• regexp_tokenize to tokenize a string or document based on a regex pattern.\n",
    "\n",
    "• TweetTokenizer for tweet tokenization, to separate hashtags, mentions, and re- peated punctuation marks!!!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['I', 'don', '’', 't', 'like', 'Martin', '’', 's', 'gloves', '.']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "word_tokenize('I don’t like Martin’s gloves.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk . tokenize import word_tokenize , sent_tokenize , regexp_tokenize , TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['thanks', '@blabla'], ['#NLP', 'is', 'fun', '!']]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "word_tokenize ( \"Hi there ! \" )\n",
    "sent_tokenize ( ' Hello world . I love HK! ' )\n",
    "# Make set of unique tokens .\n",
    "set(word_tokenize( 'I love HK. I love NYC' ))\n",
    "# Tokenize based on regular expression .\n",
    "regexp_tokenize ( 'SOLDIER #1: Found them? ' , r'(\\w+|#\\d|\\?|!)' )\n",
    "# Find hastags in tweets .\n",
    "regexp_tokenize('This is a great #NLP exercise.', r'#\\w+')\n",
    "# Find mentions and hashtags .\n",
    "regexp_tokenize ( ' great #NLP exercise from @blabla . ' , r'[#@]\\w+' ) \n",
    "tknzr = TweetTokenizer() # Create instance of TweetTokenizer. \n",
    "[tknzr.tokenize(t) for t in ['thanks @blabla', '#NLP is fun!']]"
   ]
  },
  {
   "source": [
    "## Whitespace elimination \n",
    "\n",
    "means that you remove excessive whitespace so that you end up having a contiguous sequence of words. Oftentimes you do not have to worry too much about whitespace elimination as it is done by the tokenizer. But if you would like to do it yourself you could use some of the following examples (the last example uses regular expressions, see Section 10.3 starting on page 73):\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " x = '    Test of leading and trailing whitespace   \\n   '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Test of leading and trailing whitespace'"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "x.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Test', 'of', 'leading', 'and', 'trailing', 'whitespace']"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Test of leading and trailing whitespace'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "\" \".join(x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Test of leading and trailing whitespace \n Test of leading and trailing whitespace   \n   \n    Test of leading and trailing whitespace   \n   \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(re.sub('\\s+', ' ', x))\n",
    "# get rid of leading space with ^\n",
    "print(re.sub('^\\s+', ' ', x))\n",
    "\n",
    "# get rid of trailing space with $ sign\n",
    "print(re.sub('^\\s$', ' ', x))"
   ]
  },
  {
   "source": [
    "# from Text to Numbers\n",
    "\n",
    "## Binary Vector\n",
    "\n",
    "The simplest way is to simply check whether each word is contained in the text docu- ment. If a word occurs in the document, the word gets a value of one, if it is absent it gets zero. So each document can be represented by a binary vector, where each vector element corresponds to a different word, indicating whether or not this word occurs in the document.\n",
    "\n",
    "## Bag-of-Words (BoW)\n",
    "\n",
    "Another simple way is to count the words in each document, which is the bag-of-words text representation (see also Chapter 15 starting on page 125). This way, you can think of one text document being represented by a vector containing the word counts. And several documents are just a collection of several such vectors.\n",
    "\n",
    "\n",
    "## Word Weighting\n",
    "\n",
    "Another way is to use a different word weighting. The basic idea is that instead of simply using the word counts, we give those words that are more important a higher weight while reducing the weight on the unimportant words.\n",
    "A commonly-used weighting scheme is “tf-idf” (see also Chapter 16 starting on page 131). There are two elements to tf-idf. The first one is that the tf-idf value increases with the number of times a word appears in a document. This is basically the same idea as before in Section 14.5.2 where we were counting words using bag-of-words. The second one is that the tf-idf value decreases if the word appears very frequently in the corpus (the collection of all text documents). The basic idea is that if a word (for example “and”) appears all the time throughout all documents, it probably is not that important and should receive a lower weight.\n",
    "Although the main point of tf-idf is really just to weigh the words in a more informa- tive way, you can also use it to remove unimportant words. One way to do this is to simply get rid of words with a very low tf-idf score.\n",
    "\n",
    "## Dimensionality Reduction\n",
    "Another, maybe more “intelligent” way is to reduce the dimensionality of your word vec- tors with so-called dimensionality reduction techniques. The basic idea here is that you have a lot of words in each document, and you want to transform each document into another representation that does not use up so much space (so many words) but still contains approximately the same information content, or maybe even a more dense representation with unnecessary noise removed. We will go through a few important techniques in the following sections, specifically latent semantic analysis (LSA) in Sec- tion 14.5.5 as well as topic models (specifically LDA) in Section 14.5.6.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "defaultdict(int, {'python': 2, 'hello': 1})"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# str.count and BoW\n",
    "tokens = ['python', 'hello', 'python']\n",
    "dict([[tk, tokens.count(tk)] for tk in set(tokens)])\n",
    "\n",
    "# Ateratively\n",
    "\n",
    "from collections import defaultdict\n",
    "tokens = ['python', 'hello', 'python']\n",
    "wc = defaultdict(int)    # Initialize the word count.\n",
    "for tk in tokens:\n",
    "    wc[tk] += 1\n",
    "wc\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.        , 0.        ],\n",
       "       [0.53802897, 0.84292635],\n",
       "       [0.        , 0.        ],\n",
       "       [1.        , 0.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# Word-weighting with tf-idf\n",
    "\n",
    "# This code uses scikit-learn to calculate tf-idf. The code is very\n",
    "# similar to the one used with `CountVectorizer` (which give BoW).\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Vectorizer. `max_df` tells sklearn to ignore terms that have a\n",
    "# document frequency higher than this threshold when building the\n",
    "# vocabulary.\n",
    "v = TfidfVectorizer(stop_words='english', max_df=0.9)\n",
    "c = [                           # Minimal corpus for illustration.\n",
    "    'This is the first document.',\n",
    "    'This is the second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "# Learn the vocabulary dictionary and return the document-term\n",
    "# matrix. Tokenize and count word occurrences.\n",
    "tfidf = v.fit_transform(c)\n",
    "# Each term found by the analyzer during the fit is assigned a unique\n",
    "# integer index corresponding to a column in the resulting\n",
    "# matrix. This interpretation of the columns can be retrieved as\n",
    "# follows.\n",
    "tfidf.toarray()                   # Print the document-term matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['document', 'second']"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "tfidf.A                           # Same effect, shortcut command.\n",
    "v.get_feature_names()           # Which term is in which column?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# Inverse mapping from feature name to column index.\n",
    "v.vocabulary_.get('second')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# Mapping of documents to their tf-idf scores. Words that were not\n",
    "# seen in the training corpus are ignored.\n",
    "v.transform(['Something completely new.']).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "v.transform(['A second try.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "! 0.3260142684050715\na 0.3260142684050715\nabout 0.3260142684050715\naliens 0.3260142684050715\nand 0.3260142684050715\n"
     ]
    }
   ],
   "source": [
    "# This script illustrates how to use tf-idf with gensim.\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "corpus = ['The movie was about a spaceship and aliens. It is wonderful!',\n",
    "          'I really liked the movie. More people should go see it.',\n",
    "          'Awesome action scenes, but boring characters.']\n",
    "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "d = Dictionary(tokenized_corpus)\n",
    "bowcorpus = [d.doc2bow(doc) for doc in tokenized_corpus]\n",
    "# All the above steps are standard, but now it gets interesting:\n",
    "tfidf = TfidfModel(bowcorpus) # Create new TfidfModel from BoW corpus.\n",
    "tfidf_weights = tfidf[bowcorpus[0]] # Weights of first document.\n",
    "tfidf_weights[:5]                   # First five weights (unordered).\n",
    " # Print top five weighted words.\n",
    "sorted_tfidf_weights = \\\n",
    "    sorted(\n",
    "        tfidf_weights,\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True)\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(d.get(term_id), weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"270.591562pt\" version=\"1.1\" viewBox=\"0 0 392.14375 270.591562\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-03-05T11:47:26.317463</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 270.591562 \nL 392.14375 270.591562 \nL 392.14375 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 224.64 \nL 384.94375 224.64 \nL 384.94375 7.2 \nL 50.14375 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p6fe6f758af)\" d=\"M 65.361932 224.64 \nL 65.361932 7.2 \n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m5a893865c2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#m5a893865c2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- cat -->\n      <g transform=\"translate(68.121307 247.186875)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n        <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n        <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-99\"/>\n       <use x=\"54.980469\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"116.259766\" xlink:href=\"#DejaVuSans-116\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p6fe6f758af)\" d=\"M 217.54375 224.64 \nL 217.54375 7.2 \n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"217.54375\" xlink:href=\"#m5a893865c2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- box -->\n      <g transform=\"translate(220.303125 249.713437)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n        <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n        <path d=\"M 54.890625 54.6875 \nL 35.109375 28.078125 \nL 55.90625 0 \nL 45.3125 0 \nL 29.390625 21.484375 \nL 13.484375 0 \nL 2.875 0 \nL 24.125 28.609375 \nL 4.6875 54.6875 \nL 15.28125 54.6875 \nL 29.78125 35.203125 \nL 44.28125 54.6875 \nz\n\" id=\"DejaVuSans-120\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-98\"/>\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"121.533203\" xlink:href=\"#DejaVuSans-120\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p6fe6f758af)\" d=\"M 369.725568 224.64 \nL 369.725568 7.2 \n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.725568\" xlink:href=\"#m5a893865c2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- like -->\n      <g transform=\"translate(372.484943 248.7775)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n        <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n        <path d=\"M 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 31.109375 \nL 44.921875 54.6875 \nL 56.390625 54.6875 \nL 27.390625 29.109375 \nL 57.625 0 \nL 45.90625 0 \nL 18.109375 26.703125 \nL 18.109375 0 \nL 9.078125 0 \nz\n\" id=\"DejaVuSans-107\"/>\n        <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"55.566406\" xlink:href=\"#DejaVuSans-107\"/>\n       <use x=\"109.851562\" xlink:href=\"#DejaVuSans-101\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_4\">\n     <!-- Samples -->\n     <g transform=\"translate(196.190625 261.311875)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n       <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-83\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"124.755859\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"222.167969\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"285.644531\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"313.427734\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"374.951172\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p6fe6f758af)\" d=\"M 50.14375 214.756364 \nL 384.94375 214.756364 \n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m74437e3d90\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m74437e3d90\" y=\"214.756364\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1.00 -->\n      <g transform=\"translate(20.878125 218.555582)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p6fe6f758af)\" d=\"M 50.14375 190.047273 \nL 384.94375 190.047273 \n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m74437e3d90\" y=\"190.047273\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1.25 -->\n      <g transform=\"translate(20.878125 193.846491)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p6fe6f758af)\" d=\"M 50.14375 165.338182 \nL 384.94375 165.338182 \n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m74437e3d90\" y=\"165.338182\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 1.50 -->\n      <g transform=\"translate(20.878125 169.137401)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p6fe6f758af)\" d=\"M 50.14375 140.629091 \nL 384.94375 140.629091 \n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m74437e3d90\" y=\"140.629091\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 1.75 -->\n      <g transform=\"translate(20.878125 144.42831)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p6fe6f758af)\" d=\"M 50.14375 115.92 \nL 384.94375 115.92 \n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m74437e3d90\" y=\"115.92\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 2.00 -->\n      <g transform=\"translate(20.878125 119.719219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p6fe6f758af)\" d=\"M 50.14375 91.210909 \nL 384.94375 91.210909 \n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m74437e3d90\" y=\"91.210909\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 2.25 -->\n      <g transform=\"translate(20.878125 95.010128)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p6fe6f758af)\" d=\"M 50.14375 66.501818 \nL 384.94375 66.501818 \n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m74437e3d90\" y=\"66.501818\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 2.50 -->\n      <g transform=\"translate(20.878125 70.301037)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p6fe6f758af)\" d=\"M 50.14375 41.792727 \nL 384.94375 41.792727 \n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m74437e3d90\" y=\"41.792727\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 2.75 -->\n      <g transform=\"translate(20.878125 45.591946)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p6fe6f758af)\" d=\"M 50.14375 17.083636 \nL 384.94375 17.083636 \n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m74437e3d90\" y=\"17.083636\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 3.00 -->\n      <g transform=\"translate(20.878125 20.882855)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- Counts -->\n     <g transform=\"translate(14.798438 133.373125)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 64.40625 67.28125 \nL 64.40625 56.890625 \nQ 59.421875 61.53125 53.78125 63.8125 \nQ 48.140625 66.109375 41.796875 66.109375 \nQ 29.296875 66.109375 22.65625 58.46875 \nQ 16.015625 50.828125 16.015625 36.375 \nQ 16.015625 21.96875 22.65625 14.328125 \nQ 29.296875 6.6875 41.796875 6.6875 \nQ 48.140625 6.6875 53.78125 8.984375 \nQ 59.421875 11.28125 64.40625 15.921875 \nL 64.40625 5.609375 \nQ 59.234375 2.09375 53.4375 0.328125 \nQ 47.65625 -1.421875 41.21875 -1.421875 \nQ 24.65625 -1.421875 15.125 8.703125 \nQ 5.609375 18.84375 5.609375 36.375 \nQ 5.609375 53.953125 15.125 64.078125 \nQ 24.65625 74.21875 41.21875 74.21875 \nQ 47.75 74.21875 53.53125 72.484375 \nQ 59.328125 70.75 64.40625 67.28125 \nz\n\" id=\"DejaVuSans-67\"/>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"131.005859\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"194.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"257.763672\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"296.972656\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_25\">\n    <path clip-path=\"url(#p6fe6f758af)\" d=\"M 65.361932 17.083636 \nL 217.54375 17.083636 \nL 369.725568 214.756364 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 224.64 \nL 50.14375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 224.64 \nL 384.94375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 224.64 \nL 384.94375 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 7.2 \nL 384.94375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6fe6f758af\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Samples', ylabel='Counts'>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# This file shows simple examples how to calculate bag-of-words.\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "text = '''The cat is in the box.\n",
    "          The cat likes the box.\n",
    "          The box is over the cat.'''\n",
    "# Simple example without any preprocessing.\n",
    "c = Counter(word_tokenize(text))\n",
    "c                               # View the counts.\n",
    "c.values()                      # Only the word count numbers.\n",
    "c.most_common(2)                # The two most common words.\n",
    "list(c.elements())              # All the words.\n",
    "# Convert to lowercase and only keep alphabetic (remove punctuation etc.)\n",
    "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
    "# Keep words that are not stopwords (i.e. remove stopwords).\n",
    "no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
    "Counter(no_stops).most_common(2)\n",
    "# Now we lemmatize the words (similar to stemming).\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatized = [wnl.lemmatize(t) for t in no_stops]\n",
    "Counter(lemmatized).most_common(2)\n",
    "\n",
    "# An alternative that is native to NLTK but actually an extension of\n",
    "# the `Counter` class. Another advantage is that it can plot easily\n",
    "# using the matplotlib package.\n",
    "from nltk.probability import FreqDist\n",
    "fd = FreqDist(lemmatized)\n",
    "fd                           # Looks like `Counter`.\n",
    "fd.plot(10)                  # Specify how many words to plot at most.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 1]])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# This code uses scikit-learn to calculate bag-of-words\n",
    "# (BOW). `CountVectorizer` implements both tokenization and occurrence\n",
    "# counting in a single class.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "v = CountVectorizer(stop_words='english') # Vectorizer.\n",
    "c = [                           # Minimal corpus for illustration.\n",
    "    'This is the first document.',\n",
    "    'This is the second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "# Learn the vocabulary dictionary and return the document-term\n",
    "# matrix. Tokenize and count word occurrences.\n",
    "bow = v.fit_transform(c)\n",
    "# Each term found by the analyzer during the fit is assigned a unique\n",
    "# integer index corresponding to a column in the resulting\n",
    "# matrix. This interpretation of the columns can be retrieved as\n",
    "# follows.\n",
    "bow.toarray()                   # Print the document-term matrix.\n",
    "bow.A                           # Same effect, shortcut command.\n",
    "v.get_feature_names()           # Which term is in which column?\n",
    "# Inverse mapping from feature name to column index.\n",
    "v.vocabulary_.get('second')\n",
    "# Mapping of documents to BOW. Words that were not seen in the\n",
    "# training corpus are ignored.\n",
    "v.transform(['Something completely new.']).toarray()\n",
    "v.transform(['A second try.']).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(4, 2), (6, 2), (0, 1), (1, 1), (2, 1), (3, 1), (5, 1), (7, 1), (8, 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# Bag-of-words using gensim.\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "# This is an example corpus consisting of movie reviews. You can think\n",
    "# of each movie review as a separate text document.\n",
    "cp = \\\n",
    "    ['The movie was about a spaceship and aliens. The movie is wonderful!',\n",
    "     'I really liked the movie. More people should go see it.',\n",
    "     'Awesome action scenes, but boring characters.']\n",
    "# Create tokenized corpus. Very basic preprocessing. Usually you would\n",
    "# do more work here.\n",
    "cp = [word_tokenize(doc.lower()) for doc in cp]\n",
    "cp = [[token for token in doc if token.isalnum() and len(token) > 1]\n",
    "      for doc in cp]\n",
    "\n",
    "# Pass to gensim `Dictionary` class. This assigns to each token\n",
    "# (e.g. word) a unique integer ID. Later on we will just work with\n",
    "# those IDs instead of the tokens directly because it is\n",
    "# computationally easier to handle (there is a one-to-one mapping\n",
    "# between both, so we are not losing any information). The reason why\n",
    "# we use a dictionary is that it gives us a list of words we are\n",
    "# interested in examining further. If a word is not in the dictionary\n",
    "# but occurs in a document, it will be ignored by gensim.\n",
    "d = Dictionary(cp)\n",
    "d.token2id  # Like dict(d); show mapping between tokens and their IDs.\n",
    "d.token2id.get('awesome')       # What's the ID for 'awesome'?\n",
    "d.get(0)                        # Which token has ID=0?\n",
    "d.dfs # In how many documents does each token appear? (Document frequency).\n",
    "\n",
    "# For a single document, we can now calculate the token frequencies\n",
    "# using the dictionary we just created. \"Calculating token\n",
    "# frequencies\" means we're counting words.\n",
    "d.doc2bow(cp[2])\n",
    "\n",
    "# Next, using the dictionary we just created, we build a gensim\n",
    "# corpus, which is just a bag-of-words representation of the original\n",
    "# corpus. This is a nested list (a list of lists), where each list\n",
    "# corresponds to a document. Inside each list we have tuples in the\n",
    "# form\n",
    "#\n",
    "# (token_ID, token_frequency).\n",
    "#\n",
    "# So all we are really doing here is counting words.\n",
    "cp = [d.doc2bow(doc) for doc in cp]\n",
    "# This gensim corpus can now be saved, updated, and reused using tools\n",
    "# from gensim. The dictionary can also be saved and updaed as well,\n",
    "# e.g. if we need to add more words later on.\n",
    "\n",
    "# Print the first three token IDs and their frequency counts from the\n",
    "# first document.\n",
    "cp[0][:3]\n",
    "# For the first document, sort the tokens according to their\n",
    "# frequency, with the most frequent tokens coming first.\n",
    "sorted(\n",
    "    cp[0],                 # First document.\n",
    "    key = lambda x: x[1],  # Sort by token_frequency (second element).\n",
    "    reverse = True)        # Most frequent first.\n"
   ]
  },
  {
   "source": [
    "## Latent Semantic Analysis (LSA)\n",
    "\n",
    "There are many methods out there to achieve dimensionality reduction, one being singular-value decomposition (SVD).\n",
    "\n",
    "\n",
    "There are a few things to note about SVD. First of all, you can apply the SVD on a term- document matrix containing raw word counts (i.e. BoW) or on a term-document matrix containing weighted word counts, e.g. tf-idf. Second, it is important to realize that SVD is unsupervised, i.e. no human input is necessary for SVD to reduce the dimensionality of the text documents.\n",
    "\n",
    "\n",
    "The basic idea of SVD is to pick up latent semantic structure, which means that there is some structure about the meaning of words hidden in the word counts that you crystallize with SVD. For this reason, any analysis that you perform after applying the SVD (the SVD is used to extract the latent semantics) is also called latent semantic analysis (LSA) or latent semantic indexing (LSI). In addition to the following para- graphs, we discuss LSA in more detail in Section 19.6 starting on page 150.\n",
    "\n",
    "## Vector Space Models and Word Embeddings\n",
    "\n",
    "Another way to transform text to vectors are so-called word embeddings. The idea is to involve a mathematical embedding from a space with one dimension per word (i.e. a very high-dimensional space) to a continuous vector space with much lower dimensionality. The goal is to have a relatively dense representation to avoid issues with sparsity and to have a higher information denseness. Words sharing common contexts should be located in close proximity to another in that space.\n",
    "A popular example is word2vec, which is an unsupervised model that transforms words based on their co-occurrence into a higher-dimensional vector space. For exam- ple, the vector for “king” minus the vector for “man” plus the vector for “women” should be close to the vector for “queen.” Another popular model is GloVe which is similar to word2vec. For our purposes a more relevant model is doc2vec, which is based on word2vec but works on whole documents instead of words. Another popular way to en- code sentences or short paragraphs is Google’s Universal Sentence Encoder. We dis- cuss word embeddings in more detail in Chapter 19 starting on page 143."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}