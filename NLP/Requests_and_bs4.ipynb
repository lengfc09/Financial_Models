{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a5e30402f147a97cac5d558dbfea4ff3f916a58d454bb2c6efc731faf3dd0927"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script shows how to use the requests package to scrape a page\n",
    "# from Wikipedia.\n",
    "import requests\n",
    "# Get the response. It is always a good idea to set the `timeout`\n",
    "# argument in production code. Otherwise your program may hang\n",
    "# indefinitely.\n",
    "r = \\\n",
    "    requests.get(\n",
    "        'https://en.wikipedia.org/wiki/Natural_language_processing',\n",
    "        timeout=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# The following code block is strictly speaking not necessary, but it\n",
    "# helps you to better understand the response you got.\n",
    "r.raise_for_status()            # Ensure we notice bad responses.\n",
    "r.status_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'text/html; charset=UTF-8'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "r.headers['content-type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'UTF-8'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "r.encoding\n"
   ]
  },
  {
   "source": [
    "print(r.text)                   # `print` gives nicer output for HTML.\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r.text.encode('utf-8')          # Use specific encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()            # Doesn't work in this example since no JSON data.\n",
    "# Finally we write the response content to file.\n",
    "with open('data-wikipedia-NLP.html', mode='wb') as fd:\n",
    "    fd.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script shows how to download and parse a Wikipedia page using\n",
    "# Requests and Beautiful Soup. We also use the `re` module for regular\n",
    "# expressions.\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "source": [
    "r = \\\n",
    "    requests.get(\n",
    "        'https://en.wikipedia.org/wiki/Natural_language_processing',\n",
    "        timeout=3)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 10,
   "outputs": []
  },
  {
   "source": [
    "s = BeautifulSoup(r.text, 'lxml')  # Use `lxml` to parse the webpage.\n",
    "print(s.prettify())               # Take a look at the parsed webpage.\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Extract all the text from the webpage. THIS IS WHAT YOU NEED MOST\n",
    "# OFTEN FOR NLP AND TEXT ANALYTICS, unless you need to extract only\n",
    "# part of the webpage.\n",
    "s.get_text()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'head'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Ways to navigate the data structure.\n",
    "s.title\n",
    "s.title.name\n",
    "s.title.string\n",
    "s.title.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<p><b>Natural language processing</b> (<b>NLP</b>) is a subfield of <a href=\"/wiki/Linguistics\" title=\"Linguistics\">linguistics</a>, <a href=\"/wiki/Computer_science\" title=\"Computer science\">computer science</a>, and <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of <a href=\"/wiki/Natural_language\" title=\"Natural language\">natural language</a> data.  The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. \n",
       "</p>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "s.p               # First 'p' tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.  The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. \\n'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "s.p.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<a id=\"top\"></a>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "s.a               # First 'a' tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<footer class=\"mw-footer\" id=\"footer\" role=\"contentinfo\">\n",
       "<ul id=\"footer-info\">\n",
       "<li id=\"footer-info-lastmod\"> This page was last edited on 26 February 2021, at 12:02<span class=\"anonymous-show\"> (UTC)</span>.</li>\n",
       "<li id=\"footer-info-copyright\">Text is available under the <a href=\"//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\" rel=\"license\">Creative Commons Attribution-ShareAlike License</a><a href=\"//creativecommons.org/licenses/by-sa/3.0/\" rel=\"license\" style=\"display:none;\"></a>;\n",
       "additional terms may apply.  By using this site, you agree to the <a href=\"//foundation.wikimedia.org/wiki/Terms_of_Use\">Terms of Use</a> and <a href=\"//foundation.wikimedia.org/wiki/Privacy_policy\">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href=\"//www.wikimediafoundation.org/\">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\n",
       "</ul>\n",
       "<ul id=\"footer-places\">\n",
       "<li id=\"footer-places-privacy\"><a class=\"extiw\" href=\"https://foundation.wikimedia.org/wiki/Privacy_policy\" title=\"wmf:Privacy policy\">Privacy policy</a></li>\n",
       "<li id=\"footer-places-about\"><a href=\"/wiki/Wikipedia:About\" title=\"Wikipedia:About\">About Wikipedia</a></li>\n",
       "<li id=\"footer-places-disclaimer\"><a href=\"/wiki/Wikipedia:General_disclaimer\" title=\"Wikipedia:General disclaimer\">Disclaimers</a></li>\n",
       "<li id=\"footer-places-contact\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\">Contact Wikipedia</a></li>\n",
       "<li id=\"footer-places-mobileview\"><a class=\"noprint stopMobileRedirectToggle\" href=\"//en.m.wikipedia.org/w/index.php?title=Natural_language_processing&amp;mobileaction=toggle_view_mobile\">Mobile view</a></li>\n",
       "<li id=\"footer-places-developers\"><a href=\"https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\">Developers</a></li>\n",
       "<li id=\"footer-places-statslink\"><a href=\"https://stats.wikimedia.org/#/en.wikipedia.org\">Statistics</a></li>\n",
       "<li id=\"footer-places-cookiestatement\"><a href=\"https://foundation.wikimedia.org/wiki/Cookie_statement\">Cookie statement</a></li>\n",
       "</ul>\n",
       "<ul class=\"noprint\" id=\"footer-icons\">\n",
       "<li id=\"footer-copyrightico\"><a href=\"https://wikimediafoundation.org/\"><img alt=\"Wikimedia Foundation\" height=\"31\" loading=\"lazy\" src=\"/static/images/footer/wikimedia-button.png\" srcset=\"/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x\" width=\"88\"/></a></li>\n",
       "<li id=\"footer-poweredbyico\"><a href=\"https://www.mediawiki.org/\"><img alt=\"Powered by MediaWiki\" height=\"31\" loading=\"lazy\" src=\"/static/images/footer/poweredby_mediawiki_88x31.png\" srcset=\"/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x\" width=\"88\"/></a></li>\n",
       "</ul>\n",
       "<div style=\"clear: both;\"></div>\n",
       "</footer>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# The difference between the `find` and the `find_all` methods is that\n",
    "# the former only finds the FIRST child of this tag matching the given\n",
    "# criterial, while the latter gets ALL of them.\n",
    "s.find(id='footer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<div style=\"clear: both;\"></div>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "s.find(style='clear: both;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all 'a' tags.\n",
    "atags = s.find_all('a')           # Find all `<a ...>...</a>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<a id=\"top\"></a>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "atags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<a class=\"image\" href=\"/wiki/File:Automated_online_assistant.png\"><img alt=\"\" class=\"thumbimage\" data-file-height=\"501\" data-file-width=\"400\" decoding=\"async\" height=\"251\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/8/8b/Automated_online_assistant.png/200px-Automated_online_assistant.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/8/8b/Automated_online_assistant.png/300px-Automated_online_assistant.png 1.5x, //upload.wikimedia.org/wikipedia/commons/8/8b/Automated_online_assistant.png 2x\" width=\"200\"/></a>"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "atags[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "atags[3].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/wiki/File:Automated_online_assistant.png'"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "atags[3].get('href')  # Get the actual `href` attribute (i.e. the URL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'#cite_note-Kongthon-1',\n",
       " '#mw-head',\n",
       " '#searchInput',\n",
       " '/wiki/Automated_online_assistant',\n",
       " '/wiki/Computer_science',\n",
       " '/wiki/Customer_service',\n",
       " '/wiki/File:Automated_online_assistant.png',\n",
       " '/wiki/Linguistics',\n",
       " None}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# If we want to get all `href` attributes, we loop over all `a` tags.\n",
    "{tag.get('href') for tag in s.find_all('a')[:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'b', 'bdi', 'body', 'br'}"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# Find all tags whose names start with the letter 'b' (in this case\n",
    "# 'body', 'b', and 'br').\n",
    "{tag.name for tag in s.find_all(re.compile('^b'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'annotation',\n",
       " 'cite',\n",
       " 'dt',\n",
       " 'footer',\n",
       " 'html',\n",
       " 'input',\n",
       " 'math',\n",
       " 'meta',\n",
       " 'mstyle',\n",
       " 'noscript',\n",
       " 'script',\n",
       " 'semantics',\n",
       " 'style',\n",
       " 'table',\n",
       " 'tbody',\n",
       " 'td',\n",
       " 'th',\n",
       " 'title',\n",
       " 'tr'}"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# Find all tags whose name contains the letter 't'.\n",
    "{tag.name for tag in s.find_all(re.compile('t'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'a', 'body'}"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# We can also pass a list to the `find_all` method, in which case bs4\n",
    "# allows a string match against any item in that list.\n",
    "{tag.name for tag in s.find_all(['a', 'body'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'head', 'html', 'link', 'meta', 'script', 'title'}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Find all the tags in the document, but none of the text\n",
    "# strings. `True` matches anything it can.\n",
    "{tag.name for tag in s.find_all(True)[:10]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Automatically translate text from one human language to another.  This is one of the most difficult problems, and is a member of a class of problems colloquially termed \"AI-complete\", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly.']"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# how to use selector\n",
    "[tag.get_text() for tag in s.select(\n",
    "    \"#mw-content-text > div.mw-parser-output > dl:nth-child(56) > dd:nth-child(12)\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}